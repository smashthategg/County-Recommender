{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c46f9f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          area_name state  Population_Estimate     FP  Poverty_Percent  \\\n",
      "0  ABBEVILLE COUNTY    SC              24434.0  45001             15.3   \n",
      "1     ACADIA PARISH    LA              56489.0  22001             25.0   \n",
      "2   ACCOMACK COUNTY    VA              33239.0  51001             17.3   \n",
      "3        ADA COUNTY    ID             524673.0  16001              8.0   \n",
      "4      ADAIR COUNTY    IA               7389.0  19001              9.8   \n",
      "\n",
      "   Bachelor_Or_Higher  Unemployment_Rate  Median_Income  Avg_Temp  \\\n",
      "0              3332.0                3.7        50325.0      64.4   \n",
      "1              5071.0                3.7        42981.0      70.3   \n",
      "2              5314.0                3.5        56357.0      59.1   \n",
      "3            153355.0                2.7        87748.0      52.8   \n",
      "4              1052.0                2.4        66997.0      50.6   \n",
      "\n",
      "   Avg_Precipitation  Crime_Rate_Per_100000  \n",
      "0              37.73                 511.86  \n",
      "1              62.29                 163.98  \n",
      "2              40.81                 190.06  \n",
      "3              13.34                 206.50  \n",
      "4              35.49                  66.92  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "# Load data\n",
    "pop = pd.read_csv(\"data/PopulationEstimates.csv\", encoding='latin-1', dtype={'fipstxt': str}).rename(columns=lambda x: x.strip().lower().replace(' ', '_'))\n",
    "poverty = pd.read_csv(\"data/Poverty2023.csv\", encoding='latin-1').rename(columns=lambda x: x.strip().lower().replace(' ', '_'))\n",
    "edu = pd.read_csv(\"data/Education2023.csv\", encoding='latin-1').rename(columns=lambda x: x.strip().lower().replace(' ', '_'))\n",
    "unemp = pd.read_csv(\"data/Unemployment2023.csv\", encoding='latin-1').rename(columns=lambda x: x.strip().lower().replace(' ', '_'))\n",
    "crime_rate = pd.read_csv('data/crime_rate.csv', encoding='latin-1')\n",
    "temperature = pd.read_csv('data/temperature.csv', encoding='latin-1', comment='#')\n",
    "precipitation = pd.read_csv('data/precipitation.csv', encoding='latin-1', comment='#')\n",
    "type(pop['fipstxt'])\n",
    "pop['fipstxt'] = pop['fipstxt'].apply(lambda x: f\"{int(x):05d}\" if pd.notnull(x) else np.nan)\n",
    "pop['fipstxt'] = pop['fipstxt'].str.zfill(5)\n",
    "\n",
    "# First subset by the attribute we want, then select the state and area name and create a new variable to store values in\n",
    "pop = pop[pop['attribute'] == 'POP_ESTIMATE_2023'][['state', 'area_name', 'value','fipstxt']].rename(columns={'value': 'Population_Estimate','fipstxt': 'FP'})\n",
    "poverty = poverty[poverty['attribute'] == 'PCTPOVALL_2023'][['stabr', 'area_name', 'value']].rename(columns={'stabr': 'state', 'value': 'Poverty_Percent'})\n",
    "edu = edu[edu['attribute'] == \"Bachelor's degree or higher, 2019-23\"][['state', 'area_name', 'value']].rename(columns={'value': 'Bachelor_Or_Higher'})\n",
    "\n",
    "precipitation['state'] = precipitation['ID'].str[:2]\n",
    "precipitation_cleaned = precipitation[['state', 'Value', 'Name']].rename(columns={'Value': 'Avg_Precipitation', 'Name': 'area_name'})\n",
    "\n",
    "temperature['state'] = temperature['ID'].str[:2]\n",
    "temperature_cleaned = temperature[['state', 'Value', 'Name']].rename(columns={'Value': 'Avg_Temp', 'Name': 'area_name'})\n",
    "\n",
    "crime_cleaned = crime_rate.copy()\n",
    "crime_cleaned[['area_name', 'state']] = crime_cleaned['county_name'].str.split(\", \", expand=True)\n",
    "crime_cleaned['area_name'] = crime_cleaned['area_name'].str.upper()\n",
    "crime_cleaned['state'] = crime_cleaned['state'].str.upper()\n",
    "crime_cleaned['Crime_Rate_Per_100000'] = crime_cleaned['crime_rate_per_100000'].round(2)\n",
    "crime_cleaned = crime_cleaned[['area_name', 'state', 'Crime_Rate_Per_100000']]\n",
    "\n",
    "# Removing the , XX at the end of area names in unemp\n",
    "unemp['area_name'] = unemp['area_name'].str.replace(\",\\\\s*\\\\w{2}$\", \"\", regex=True)\n",
    "medianincome = unemp[unemp['attribute'] == 'Median_Household_Income_2022'][['state', 'area_name', 'value']].rename(columns={'value': 'Median_Income'})\n",
    "unemp = unemp[unemp['attribute'] == 'Unemployment_rate_2023'][['state', 'area_name', 'value']].rename(columns={'value': 'Unemployment_Rate'})\n",
    "\n",
    "# Combines datasets into a list to be used via full join\n",
    "data_list = [pop, poverty, edu, unemp, medianincome, temperature_cleaned, precipitation_cleaned]\n",
    "\n",
    "# Combines the data, normalizes the capitalization, and collapses any duplicate entries after basic data cleaning, and finally removes rows with missing values\n",
    "combined_data = reduce(lambda left, right: pd.merge(left, right, on=['area_name', 'state'], how='outer'), data_list)\n",
    "combined_data['area_name'] = combined_data['area_name'].str.upper()\n",
    "combined_data['state'] = combined_data['state'].str.upper()\n",
    "combined_data = combined_data.merge(crime_cleaned, on=['area_name', 'state'], how='outer')\n",
    "\n",
    "combined_data['area_name'] = combined_data['area_name'].str.replace(r\"/.*\", \"\", regex=True).str.strip()\n",
    "combined_data = combined_data.groupby(['area_name', 'state']).agg(lambda x: x.dropna().iloc[0] if not x.dropna().empty else np.nan).reset_index()\n",
    "combined_data = combined_data.dropna()\n",
    "\n",
    "print(combined_data.head())\n",
    "# Save the combined data to a CSV file\n",
    "combined_data.to_csv(\"CombinedData.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea752b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      FP  Walkability\n",
      "0  01001     3.267533\n",
      "1  01003     4.104647\n",
      "2  01005     3.703383\n",
      "3  01007     4.165608\n",
      "4  01009     4.360420\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "# Load the appropriate layer\n",
    "gdf = gpd.read_file(\"data/Natl_WI.gdb\", layer=\"NationalWalkabilityIndex\")  # Replace with actual layer name\n",
    "full_data = gdf.drop(columns=[\"geometry\"])  # drop geometry if you just want tabular data\n",
    "# We will use STATEFP and COUNTYFP as a \"key\" to merge with our combined dataset.\n",
    "# There are 220,000 observations for 3,000 counties. \n",
    "# We need to average the walkability index (NatWalkInd) among all the blocks in each county.\n",
    "# Weighted by area (Shape_Area) of the block.\n",
    "df = full_data[[\"STATEFP\",\"COUNTYFP\",\"NatWalkInd\",\"Shape_Area\"]].copy()\n",
    "# Combine STATEFP and COUNTYFP to FP.\n",
    "df[\"FP\"] = df[\"STATEFP\"].astype(str).str.zfill(2) + df[\"COUNTYFP\"].astype(str).str.zfill(3)\n",
    "df = df.drop(columns=[\"STATEFP\",\"COUNTYFP\"])\n",
    "import numpy as np\n",
    "# Weighted average of NatWalkInd by FP\n",
    "walkability_df = (\n",
    "    df.groupby(\"FP\")[[\"NatWalkInd\", \"Shape_Area\"]]\n",
    "      .apply(lambda g: (g[\"NatWalkInd\"] * g[\"Shape_Area\"]).sum() / g[\"Shape_Area\"].sum())\n",
    "      .reset_index(name=\"Walkability\")\n",
    ")\n",
    "print(walkability_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2a70f706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3086\n",
      "          area_name state  Population_Estimate     FP  Poverty_Percent  \\\n",
      "0  ABBEVILLE COUNTY    SC              24434.0  45001             15.3   \n",
      "1     ACADIA PARISH    LA              56489.0  22001             25.0   \n",
      "2   ACCOMACK COUNTY    VA              33239.0  51001             17.3   \n",
      "3        ADA COUNTY    ID             524673.0  16001              8.0   \n",
      "4      ADAIR COUNTY    IA               7389.0  19001              9.8   \n",
      "\n",
      "   Bachelor_Or_Higher  Unemployment_Rate  Median_Income  Avg_Temp  \\\n",
      "0              3332.0                3.7        50325.0      64.4   \n",
      "1              5071.0                3.7        42981.0      70.3   \n",
      "2              5314.0                3.5        56357.0      59.1   \n",
      "3            153355.0                2.7        87748.0      52.8   \n",
      "4              1052.0                2.4        66997.0      50.6   \n",
      "\n",
      "   Avg_Precipitation  Crime_Rate_Per_100000  Walkability  \n",
      "0              37.73                 511.86     4.077864  \n",
      "1              62.29                 163.98     4.389531  \n",
      "2              40.81                 190.06     2.685684  \n",
      "3              13.34                 206.50     5.561057  \n",
      "4              35.49                  66.92     4.976618  \n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.merge(combined_data, walkability_df, on='FP', how='inner')\n",
    "print(len(merged_df))\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c3450ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             area_name state  Population_Estimate     FP  Poverty_Percent  \\\n",
      "277  BROOMFIELD COUNTY    CO              76860.0  08014              5.1   \n",
      "\n",
      "     Bachelor_Or_Higher  Unemployment_Rate  Median_Income  Avg_Temp  \\\n",
      "277             31270.0                3.0       114746.0      52.7   \n",
      "\n",
      "     Avg_Precipitation  Crime_Rate_Per_100000  Land_Area  \n",
      "277              10.21                  50.44      33.58  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3086"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "area = pd.read_csv(\"data/Land_Area.csv\", encoding='utf-8-sig')\n",
    "area.rename(columns={\"STCOU\":\"FP\", \"LND010190D\":\"Land_Area\"}, inplace=True)\n",
    "area[\"FP\"] = area[\"FP\"].astype(str).str.zfill(5)\n",
    "merged_df2 = pd.merge(combined_data, area, on='FP', how='inner')\n",
    "# There was actually one row with a land area of 0. I have edited in the Census value into the CSV beforehand\n",
    "print(merged_df2.iloc[[277]])\n",
    "\n",
    "len(merged_df2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "96959bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length is 0\n",
      "[]\n",
      "3086\n",
      "3086\n"
     ]
    }
   ],
   "source": [
    "df = merged_df2.copy()\n",
    "col=\"Land_Area\"\n",
    "indices = df[(df[col] == 0) | (df[col].isna())].index\n",
    "index_list = indices.tolist()\n",
    "print(f\"length is {len(index_list)}\")\n",
    "print(index_list)\n",
    "\n",
    "print(len(df))\n",
    "print(len(merged_df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b4a8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
