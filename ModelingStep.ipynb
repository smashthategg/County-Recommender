{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e520fc0",
   "metadata": {},
   "source": [
    "## KNN Modeling: Further Data Preprocessing\n",
    "\n",
    "We implemented a KNN-based recommender, where the user selects a reference county, and the algorithm recommends the *k* nearest counties with the smallest Euclidean distance among the featuers:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\n",
    "$$\n",
    "\n",
    "While we did consider and test other metircs like cosine similarity, we figured directionality was less important than overall proximity for this particular problem. The data is continuous and densely contained, so that the magnitude of each feature vector should be captured.\n",
    "\n",
    "Since we are working with Euclidean distance calculations, normalizing the data is crucial so that one feature does not contribute significantly more than another. We applied a min-max scaler to transform each feature to a range of $[0,1]$:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d39db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalizing the data\n",
    "numeric_cols = [\"Poverty_Percent\",\"Bachelor_Or_Higher\",\"Unemployment_Rate\",\"Median_Income\",\"Avg_Temp\",\"Avg_Precipitation\",\"Crime_Rate_Per_100000\",\"Walkability\",\"Population_Density\"]\n",
    "clean_data_norm = combined_data.copy()\n",
    "clean_data_norm.drop(columns=[\"Population_Estimate\",\"Land_Area\"], inplace=True)\n",
    "clean_data_norm[numeric_cols] = MinMaxScaler().fit_transform(combined_data[numeric_cols])\n",
    "print(clean_data_norm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286251e",
   "metadata": {},
   "source": [
    "## County-to-County Recommender\n",
    "\n",
    "After normalizing the data, we are ready to build and run the model. Note the choice of using `metric = \"euclidean\"`, reasoned earlier, and the choice of `algorithm = \"brute\"`, an exhaustive search algorithm which computes the distance between the input and every vector in the data set. This simple and straightforward approach performs better with high dimensionality, and is affordable given our relatively small size of the data set (~3000 counties). Our first system benefits from being intuitive (finding the most numerically similar counties) and easy to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Fitting the KNN model\n",
    "data_matrix = clean_data_norm[numeric_cols]\n",
    "model_knn = NearestNeighbors(metric = \"euclidean\", algorithm = \"brute\")\n",
    "model_knn.fit(data_matrix)\n",
    "\n",
    "# Testing the model to return top 5 county recommendations (n_neighbors = 6 but we disregard the first recommendation which is the input county itself).\n",
    "np.random.seed(1)\n",
    "query_no = np.random.choice(clean_data_norm.shape[0]) # random county index\n",
    "print(f\"We will find recommendations for the county {clean_data_norm.iloc[query_no]['area_name'].title()}, {clean_data_norm.iloc[query_no]['state']}.\")\n",
    "distances, indices = model_knn.kneighbors(data_matrix.iloc[query_no, :].values.reshape(1, -1), n_neighbors=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbcc1ee",
   "metadata": {},
   "source": [
    "While nice to have, it is clear that this model lacks flexibility. Our pure similarity model treats all features as equally important. However, different users may prioritize certain attributes over others (ex. a retired person may not care at all for the state of economy like `Median_Income` or `Unemployment_Rate`). In addition, users may not always have a preferred reference county to work with.\n",
    "\n",
    "## User Preference-based Recommender\n",
    "\n",
    "### Creating User Embeddings\n",
    "\n",
    "To fix this weakness, we extended our system to incorporate user-defined preferences over different feature categories, as a **weighted** KNN model. Through UI-based survey questions, users build their imagined ideal county by indicating their preferences for a \"higher quantity\" of each feature on a scale of 1-5. These scores are mapped back to their corresponding features and linearly transformed to $[0,1]$, the same normalized space as in the data set:\n",
    "\n",
    "| Original Score (1–5) | Normalized (0–1) |\n",
    "|----------------------|------------------|\n",
    "| 1                    | 0.00             |\n",
    "| 2                    | 0.25             |\n",
    "| 3                    | 0.50             |\n",
    "| 4                    | 0.75             |\n",
    "| 5                    | 1.00             |\n",
    "\n",
    "\n",
    "In doing so, we have constructed a user embedding that can be imagined as a new county vector tailored to the user's interests. It requires no original county, and offers much more personalization and flexibility, and still integrates smoothly into the old KNN model.\n",
    "\n",
    "Furthermore, users provide weights through similar survey questions asking the importance of each feature on a scale of 1-5, and the answers are stored internally as a weight vector.\n",
    "\n",
    "### Adding Weights\n",
    "\n",
    "In similar fashion, users provide weights for each feature as an importance score from 1-5. Storing this internally as a weight vector mapped to each feature, we can then multiply each feature's numeric value by its corresponding user-defined weight before calculating the Euclidean distance in the model. Without a practical method of evaluating our models, we settled on an arbitrary and simple weighted Euclidean distance function as follows:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{\\sum_{i=1}^{n} \\sqrt{w_i} (p_i - q_i)^2}\n",
    "$$\n",
    "\n",
    "where $w_i$ is also linearly scaled down to [0,1]. Higher-priority (high $w_i$) features contribute more to the summated distance, allowing users to place an emphasis on certain features. To incorporate this into the model, we simply multiply both the user embedding and the entire data set by the square root of the weight vector, and that accomplishes the same thing mathematically. Full implementation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the user embedding and weight vector\n",
    "def get_user_preferences():    \n",
    "# Map columns to user-friendly questions\n",
    "    features = {\n",
    "        \"Poverty_Percent\": (\"I prefer living in an area with a lower poverty rate.\", \"poverty rate\"),\n",
    "        \"Bachelor_Or_Higher\": (\"I prefer living in areas with highly educated populations\", \"higher education\"),\n",
    "        \"Unemployment_Rate\": (\"I prefer living in regions with lower unemployment rates\", \"unemployment rate\"),\n",
    "        \"Median_Income\": (\"I prefer living in generally more affluent areas\", \"median income\"),\n",
    "        \"Avg_Temp\": (\"I prefer warmer climates\", \"temperature\"),\n",
    "        \"Avg_Precipitation\": (\"I prefer seeing less rain and less snow\", \"rain/snow\"),\n",
    "        \"Crime_Rate_Per_100000\": (\"I prefer living in an area with a lower crime rate\", \"crime rate\"), \n",
    "        \"Walkability\": (\"I prefer walking over other modes of transportation\", \"walkability\"),\n",
    "        \"Population_Density\": (\"I prefer living in more densely populated regions\", \"urban lifestyle\")\n",
    "    }\n",
    "\n",
    "    pref_values = []\n",
    "    importance_values = []\n",
    "\n",
    "    for col, (pref_question, feature) in features.items():\n",
    "        while True:\n",
    "            try:\n",
    "                # Preference question\n",
    "                pref = int(input(f\"On a scale of 1-5, how much do you agree with the statement: {pref_question}\"))\n",
    "                if 1 <= pref <= 5:\n",
    "                    # We have to invert to make sense with some of our questions.\n",
    "                    if col in [\"Poverty_Percent\",\"Unemployment_Rate\",\"Avg_Precipitation\",\"Crime_Rate_Per_100000\"]:\n",
    "                        pref = 6 - pref\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please enter a number from 1 to 5.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter an integer.\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Preference question\n",
    "                importance = int(input(f\"On a scale of 1-5, how important is {feature} when choosing a place to live?\"))\n",
    "                if 1 <= importance <= 5:\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Please enter a number from 1 to 5.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter an integer.\")\n",
    "        \n",
    "        pref_values.append(pref)\n",
    "        importance_values.append(importance)\n",
    "\n",
    "    # Normalize both to [0,1]\n",
    "    pref_values = np.array(pref_values).reshape(-1, 1)\n",
    "    importance_values = np.array(importance_values).reshape(-1, 1)\n",
    "\n",
    "    user_embedding = (pref_values.flatten() - 1) / 4  # 1 maps to 0, 5 maps to 1\n",
    "    weight_vector = (importance_values.flatten() - 1) / 4\n",
    "\n",
    "    # Show output\n",
    "    print(\"\\nUser Target Embedding (1 = wanting more of):\")\n",
    "    for (col, _), val in zip(features.items(), user_embedding):\n",
    "        print(f\"{col:30}: {val:.3f}\")\n",
    "\n",
    "    print(\"\\nWeight Vector (how much each feature matters):\")\n",
    "    for (col, _), val in zip(features.items(), weight_vector):\n",
    "        print(f\"{col:30}: {val:.3f}\")\n",
    "\n",
    "    return user_embedding, weight_vector\n",
    "\n",
    "# Feeds user embedding and weight vector into model.\n",
    "def get_user_based_recommendations(target_embedding, weight_vector,n_neighbors=6):\n",
    "    # Scale features by sqrt of weights\n",
    "    sqrt_weights = np.sqrt(weight_vector)\n",
    "    weighted_data = data_matrix * sqrt_weights  # broadcasted element-wise multiplication\n",
    "\n",
    "    # Fit new KNN model on the weighted data\n",
    "    model_knn = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')\n",
    "    model_knn.fit(weighted_data.values)\n",
    "    user_embedding_weighted = target_embedding * np.sqrt(weight_vector)\n",
    "    distances, indices = model_knn.kneighbors(user_embedding_weighted.reshape(1, -1), n_neighbors=n_neighbors)\n",
    "    no = []\n",
    "    name = []\n",
    "    state = []\n",
    "    distance = []\n",
    "    population = []\n",
    "    poverty = []\n",
    "    education = []\n",
    "    unemployment = []\n",
    "    crime_rate = []\n",
    "    income = []\n",
    "    walkability = []\n",
    "    for i in range(0, len(distances.flatten())):\n",
    "        if i == 0:\n",
    "            print(f\"Recommendations for {clean_data_norm.iloc[query_no]['area_name'].title()} residents:\\n\")\n",
    "        else:\n",
    "            no.append(i)\n",
    "            name.append(county['area_name'][indices.flatten()[i]].title())\n",
    "            state.append(county['state'][indices.flatten()[i]])\n",
    "            distance.append(distances.flatten()[i])\n",
    "            population.append(county['Population_Estimate'][indices.flatten()[i]])\n",
    "            poverty.append(county['Poverty_Percent'][indices.flatten()[i]])\n",
    "            education.append(county['Bachelor_Or_Higher'][indices.flatten()[i]])\n",
    "            unemployment.append(county['Unemployment_Rate'][indices.flatten()[i]])\n",
    "            crime_rate.append(county['Crime_Rate_Per_100000'][indices.flatten()[i]])\n",
    "            income.append(county['Median_Income'][indices.flatten()[i]])\n",
    "            walkability.append(county['Walkability'][indices.flatten()[i]])\n",
    "    dic = {\"No\": no, \"County Name\": name, \"State\": state, \"Distance\": distance,\n",
    "        \"Population Estimate\": population, \"Poverty Percent\": poverty,\n",
    "        \"Bachelor's Degree or Higher\": education,\n",
    "        \"Unemployment Rate (%)\": unemployment,\n",
    "        \"Crime Rate per 100,000\": crime_rate,\n",
    "        \"Median Income\": income, \"Walkability Index\": walkability}\n",
    "    recommendation = pd.DataFrame(data=dic)\n",
    "    recommendation.set_index(\"No\", inplace=True)\n",
    "    return recommendation.style.set_properties(**{\"background-color\": \"white\", \"color\": \"black\", \"border\": \"1.5px solid black\"})\n",
    "\n",
    "def get_random_embeddings():\n",
    "    # Simulate a user giving 1-5 responses\n",
    "    random_prefs = np.random.randint(1, 6, size=9)\n",
    "    random_importance = np.random.randint(1, 6, size=9)\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    user_embedding = (random_prefs - 1) / 4\n",
    "    weight_vector = (random_importance - 1) / 4\n",
    "\n",
    "    print(\"\\nUser Target Embedding (1 = want more of):\")\n",
    "    for col, val in zip(numeric_cols, user_embedding):\n",
    "        print(f\"{col:30}: {val:.3f}\")\n",
    "\n",
    "    print(\"\\nWeight Vector (how much each feature matters):\")\n",
    "    for col, val in zip(numeric_cols, weight_vector):\n",
    "        print(f\"{col:30}: {val:.3f}\")\n",
    "\n",
    "    return user_embedding, weight_vector\n",
    "\n",
    "user_embedding, weight_vector = get_random_embeddings()\n",
    "get_user_based_recommendations(user_embedding, weight_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
